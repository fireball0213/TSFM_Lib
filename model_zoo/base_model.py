import os
import csv
import json
import time
import warnings
import random

from dotenv import load_dotenv

import numpy as np
import pandas as pd
import torch
from tqdm.auto import tqdm
from datetime import datetime

from gluonts.model import evaluate_forecasts
from gluonts.time_feature import get_seasonality
from gluonts.ev.metrics import (
    MASE,
    SMAPE,
    MeanWeightedSumQuantileLoss,
)

from data import Dataset
from utils.debug import debug_check_input_nan, debug_print_test_input, debug_forecasts

from Model_Path.model_zoo_config import Model_zoo_details, MULTIVAR_TSFM_PREFIXES

warnings.filterwarnings("ignore")
load_dotenv()  # åŠ è½½ç¯å¢ƒå˜é‡

# ====================== å…¨å±€é…ç½®ä¸å¸¸é‡ ======================

# æ•°æ®é›†å±æ€§ï¼ˆdomainã€å˜é‡ç»´åº¦ã€freq ç­‰ï¼‰
dataset_properties_map = json.load(open("Dataset_Path/dataset_properties.json", encoding="utf-8"))

# è¯„ä¼°æŒ‡æ ‡é›†åˆ
metrics = [
    MASE(),
    SMAPE(),
    MeanWeightedSumQuantileLoss(
        quantile_levels=[0.1 * i for i in range(1, 10)]
    ),
]

# æ•°æ®é›†é‡å‘½å
pretty_names = {
    "saugeenday": "saugeen",
    "temperature_rain_with_missing": "temperature_rain",
    "kdd_cup_2018_with_missing": "kdd_cup_2018",
    "car_parts_with_missing": "car_parts",
}


class BaseModel:
    def __init__(self, model_name, args, output_dir=None):
        self.args = args
        self.model_name = model_name
        if output_dir is None:
            self.output_dir = args.output_dir
        self.batch_size = args.batch_size

        self.get_save_path()
        print('Save Path: ',self.csv_file_path)

        self.done_datasets = []
        if self.args.skip_saved:
            if os.path.exists(self.csv_file_path):
                df_res = pd.read_csv(self.csv_file_path)
                if "dataset" in df_res.columns:
                    self.done_datasets = df_res["dataset"].values

                    print(f"Done {len(self.done_datasets)} datasets")
            else:
                print(f"[skip_saved] ç»“æœæ–‡ä»¶ä¸å­˜åœ¨ï¼Œå¿½ç•¥å·²å®Œæˆæ•°æ®é›†æ£€æµ‹ï¼š{self.csv_file_path}")

    # ==============================================================
    # è·¯å¾„æ„é€ ä¸ CSV å¤´éƒ¨
    # ==============================================================
    def get_save_path(self):

        os.makedirs(self.output_dir, exist_ok=True)

        if self.args.fix_context_len:
            self.output_dir = os.path.join(self.output_dir, f"cl_{self.args.context_len}")
        else:
            self.output_dir = os.path.join(self.output_dir, f"cl_original")

        if not os.path.exists(self.output_dir):
            os.makedirs(self.output_dir, exist_ok=True)
        self.csv_file_path = os.path.join(self.output_dir, "all_results.csv")

        header = [
            "dataset",
            "model",
            "eval_metrics/MASE[0.5]",
            "eval_metrics/sMAPE[0.5]",
            "eval_metrics/mean_weighted_sum_quantile_loss",
            "domain",
            "num_variates",
            "model_order"
        ]

        if not os.path.exists(self.csv_file_path):
            with open(self.csv_file_path, "w", newline="") as csvfile:
                writer = csv.writer(csvfile)
                writer.writerow(header)

    def get_predictor(self, dataset, batch_size):
        """
        å­ç±»å®ç°ï¼šåŠ è½½æ¨¡å‹ï¼Œè¿”å› predictor å¯¹è±¡
        """
        raise NotImplementedError("å­ç±»å¿…é¡»å®ç° get_predictor æ–¹æ³•")

    def _build_ds_meta(self, ds_name, term):
        """ç»Ÿä¸€è§£æ ds_nameï¼Œè¿”å› ds_key, ds_freq, ds_config, dataset_name"""
        if "/" in ds_name:
            ds_key_raw, ds_freq = ds_name.split("/")
            ds_key = pretty_names.get(ds_key_raw.lower(), ds_key_raw.lower())
        else:
            ds_key_raw = ds_name
            ds_key = pretty_names.get(ds_key_raw.lower(), ds_key_raw.lower())
            ds_freq = dataset_properties_map[ds_key]["frequency"]

        ds_config = f"{ds_key}/{ds_freq}/{term}"
        dataset_name = f"{ds_key}_{ds_freq}_{term}"
        return ds_key, ds_freq, ds_config, dataset_name

    def _decide_univariate(self, ds_name, term):
        """æ ¹æ®æ¨¡å‹ç±»å‹å’Œæ•°æ®ç»´åº¦ï¼Œå†³å®šæ˜¯å¦è½¬ä¸ºå•å˜é‡"""
        prefix = self.model_name.split("_")[0].lower()
        if (
            prefix in MULTIVAR_TSFM_PREFIXES
            or self.model_name.split("_")[-1] == "Select"
        ):
            return False

        # å…ˆç”¨ to_univariate=False æ¢æµ‹åŸå§‹ target_dim å†³å®šæ˜¯å¦éœ€è¦è½¬ä¸ºå•å˜é‡
        tmp_ds = Dataset(name=ds_name, term=term, to_univariate=False)
        return tmp_ds.target_dim != 1

    def _make_forecasts(self, dataset, dataset_name, ds_config, fixed_model_order, debug_mode):
        """
        ç»Ÿä¸€çš„é¢„æµ‹å…¥å£ï¼š
        - é€‰æ‹©å™¨ï¼šè¿”å› (forecasts, model_order)
        - éé€‰æ‹©å™¨ï¼šå†…éƒ¨å¤„ç† OOM é‡è¯•ã€debug æ‰“å°ã€NaN æ£€æŸ¥å’Œå™ªå£°æ³¨å…¥
        """
        model_order = None

        batch_size = self.batch_size
        while True:
            try:
                predictor = self.get_predictor(dataset, batch_size)
                test_input = dataset.test_data.input

                if debug_mode:
                    # æ‰“å°æ•°æ®æ ¼å¼
                    debug_print_test_input(dataset)
                    debug_check_input_nan(test_input)

                input_data = test_input
                forecasts = list(
                    tqdm(
                        predictor.predict(input_data),
                        total=len(dataset.test_data.input),
                        desc=f"Predicting {ds_config}",
                    )
                )
                break
            except torch.cuda.OutOfMemoryError:
                print(
                    f"âš ï¸ OOM at batch_size {batch_size}, "
                    f"reducing to {batch_size // 2}"
                )
                batch_size //= 2

        if debug_mode:
            debug_forecasts(forecasts)

        return forecasts, model_order

    # ==============================================================
    # ä¸»è¿è¡Œæµç¨‹
    # ==============================================================

    def run(self):
        total_time = 0
        total_memory = 0
        max_memory = 0
        fixed_model_order = None
        print(f"ğŸš€ Running {self.model_name}", )

        if len(self.done_datasets) > 0 and self.args.skip_saved:
            print(f"âœ…  Skipping...âœ…  Done with {len(self.done_datasets)} datasets. ")
        self.all_data_configs = []
        for ds_name in self.args.all_datasets:
            terms = ["short", "medium", "long"]
            for term in terms:
                # ä¸­é•¿ term åªå¯¹æŒ‡å®šæ•°æ®é›†ç”Ÿæ•ˆ
                if (term in ["medium", "long"]) and (ds_name not in self.args.med_long_datasets.split()):
                    continue

                # ç»Ÿä¸€æ„é€  ds_key / ds_freq / ds_config / dataset_name
                ds_key, ds_freq, ds_config, dataset_name = self._build_ds_meta(ds_name, term)
                self.all_data_configs.append(ds_config)

                if ds_config in self.done_datasets and getattr(self.args, "skip_saved", False):
                    print(f"{ds_config}.", end=" âœ…  ")
                    continue
                else:
                    print(f"\nğŸš€ Dataset: [{ds_config}]",
                          f"Model: {self.model_name}",
                          'GPU:',os.environ.get('CUDA_VISIBLE_DEVICES', 'None'),
                          'Batch_size:', self.batch_size,
                          'num_workers:', self.args.num_workers
                          )
                # ---------- æ˜¯å¦è½¬ä¸ºå•å˜é‡ ----------
                to_univariate = self._decide_univariate(ds_name, term)
                dataset = Dataset(name=ds_name, term=term, to_univariate=to_univariate)
                start_time = time.time()

                forecasts, model_order = self._make_forecasts(
                    dataset=dataset,
                    dataset_name=dataset_name,
                    ds_config=ds_config,
                    fixed_model_order=fixed_model_order,
                    debug_mode=self.args.debug_mode,
                )
                res = evaluate_forecasts(
                    forecasts=forecasts,
                    test_data=dataset.test_data,
                    metrics=metrics,
                    batch_size=1024,
                    axis=None,
                    mask_invalid_label=True,
                    allow_nan_forecast=False,
                    seasonality=get_seasonality(dataset.freq),
                )

                # ===================== è®°å½•è€—æ—¶å’Œæ˜¾å­˜ =====================
                end_time = time.time()
                elapsed = end_time - start_time
                reserved = torch.cuda.memory_reserved() / 1024 ** 2
                allocated = torch.cuda.memory_allocated() / 1024 ** 2
                memory_used = reserved + allocated

                max_memory= max(max_memory, memory_used)
                total_memory += memory_used
                total_time += elapsed

                print(f"time cost ğŸ§­ {elapsed:.2f}s",
                      f"memory-use {memory_used:.0f} MB", end=' ')

                self.save_results(res, forecasts, ds_config, dataset_name, ds_key, elapsed, memory_used,dataset, model_order)

        # ===================== è¿è¡Œç»“æŸåï¼šç»Ÿè®¡æ€»ä½“æ€§èƒ½å¹¶æ£€æŸ¥ç»“æœæ–‡ä»¶ =====================
        num_ds = len(self.all_data_configs)
        if num_ds-len(self.done_datasets) > 0 and self.args.save_pred:
            # è®¡ç®—å¹³å‡è€—æ—¶ï¼Œä¿ç•™æ•´æ•°

            average_time = total_time / max(num_ds, 1)
            average_memory = total_memory / max(num_ds, 1)

            print(f"\nğŸ§­ å·²è¿è¡Œ{num_ds}ä¸ªæ•°æ®é›†ï¼Œtotal_time:",f"{total_time:.2f}s","average_time:",f"{average_time:.2f}s",
                  "max_memory:",f"{max_memory:.0f} MB","average_memory:",f"{average_memory:.0f} MB \n",)


            # ä¿å­˜æ•´ä½“æ—¶é—´ç»Ÿè®¡åˆ° CSV æ–‡ä»¶
            time_save_filename = "results/runtime-TSFM.csv"

            if self.args.fix_context_len:
                context_tag = self.args.context_len
            else:
                context_tag = "original"
            timestamp = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
            file_exists = os.path.isfile(time_save_filename)

            row = {
                "model_name": self.model_name,
                "context_length": context_tag,
                "dataset_num": num_ds,  # æ•°æ®é›†æ•°é‡
                "total_time_s": round(total_time, 0),  # æ€»è€—æ—¶ï¼ˆç§’ï¼‰
                "average_time_s": round(average_time, 2),  # å¹³å‡è€—æ—¶ï¼ˆç§’ï¼‰
                "average_memory_MB": round(average_memory, 0),  # å¹³å‡å†…å­˜å ç”¨ï¼ˆMBï¼‰
                "timestamp": timestamp,  # æ–°å¢ï¼šæ—¶é—´æˆ³
            }

            # è¿½åŠ æ¨¡å¼
            with open(time_save_filename, "a", newline="") as csvfile:
                writer = csv.DictWriter(csvfile, fieldnames=row.keys())
                if not file_exists:
                    writer.writeheader()  # é¦–æ¬¡å†™æ–‡ä»¶æ—¶å†™å…¥åˆ—å
                writer.writerow(row)

    # ==============================================================
    # ç»“æœä¿å­˜é€»è¾‘
    # ==============================================================
    def save_results(self, res, forecasts, ds_config, dataset_name, ds_key, elapsed, memory_used, dataset=None, model_order=None):
        if self.args.save_pred:
            formatted_model_order = '[' + " ".join(map(str, model_order)) + ']' if model_order is not None else ""

            row = [
                ds_config,
                self.model_name,
                res["MASE[0.5]"][0],
                res["sMAPE[0.5]"][0],
                res["mean_weighted_sum_quantile_loss"][0],
                dataset_properties_map[ds_key]["domain"],
                dataset_properties_map[ds_key]["num_variates"],
                formatted_model_order
            ]

            with open(self.csv_file_path, "a", newline="") as csvfile:
                writer = csv.writer(csvfile)
                writer.writerow(row)

        if res is not None:
            print(
                f"{self.model_name}",
                f"MASE: {res['MASE[0.5]'][0]:.2f}",
                f"sMAPE: {res['sMAPE[0.5]'][0]:.2f}",
                f"CRPS: {res['mean_weighted_sum_quantile_loss'][0]:.2f}")

        # ä¿å­˜é¢„æµ‹ç»“æœåˆ°npyå’Œjson
        if self.args.save_pred and self.model_name.split("_")[-1] != "Select":
            # 1) æ ·æœ¬æ•°ç»„ï¼šshape = (num_series, num_samples, pred_len, num_channels)
            if hasattr(forecasts[0], "samples"):
                arrs = [fc.samples for fc in forecasts]
            elif hasattr(forecasts[0], "forecast_array"):
                arrs = [fc.forecast_array for fc in forecasts]
            else:
                print(f"forecasts[0] attributes: {dir(forecasts[0])}")
                raise ValueError("forecasts[0] does not have 'samples' or 'forecast_array' attribute")
            samples = np.stack(arrs, axis=0)

            samples_path = os.path.join(self.output_dir, f"{dataset_name}_samples.npy")

            np.save(samples_path, samples)


            # 2) ä¿å­˜å…ƒæ•°æ® + æ€§èƒ½æŒ‡æ ‡
            meta = {
                "performance": {
                    "runtime_seconds": elapsed,
                    "memory_use_mb": memory_used,
                    "batch_size": self.batch_size,
                },
                "entries": [
                    {
                        "item_id": fc.item_id,
                        "start_date": str(fc.start_date)
                    }
                    for fc in forecasts
                ]
            }
            meta_path = os.path.join(self.output_dir, f"{dataset_name}_meta.json")
            with open(meta_path, "w") as fp:
                json.dump(meta, fp)
            print(f"ğŸ‘‰ é¢„æµ‹ç»“æœnpyå’Œjsonå…ƒæ•°æ®ä¿å­˜åˆ° {self.output_dir}")
